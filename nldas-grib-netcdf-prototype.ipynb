{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is my prototyping platform for the code to extract a timeseries of data from the NLDAS GRB files and store them in NetCDF format.\n",
    "## There should be a .py script with a similar name that runs the finished code on HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import netCDF4 as nc # http://unidata.github.io/netcdf4-python/\n",
    "import pygrib as pg\n",
    "import sys\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle as pkl\n",
    "import nldas_pygrib_tools as npt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_year=1979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the NetCDF forcing data file.\n",
    "grib_dir = '/home/NearingLab/data/nldas/grib/NLDAS2.FORCING/'\n",
    "write_dir = '/home/NearingLab/data/nldas/netcdf-single-cells/'+str(nc_year)+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an example file\n",
    "fname = grib_dir + '1979/001/' + 'NLDAS_FORA0125_H.A19790101.1300.002.grb'\n",
    "#fname = grib_dir + '2019/001/' + 'NLDAS_FORA0125_H.A20190101.0000.002.grb'\n",
    "gbf_temp = pg.open(fname)\n",
    "lats = gbf_temp[1].latitudes\n",
    "lons = gbf_temp[1].longitudes\n",
    "nrows = gbf_temp[11].values.shape[0]\n",
    "ncols = gbf_temp[11].values.shape[1]\n",
    "N = nrows*ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvars = {0:'airtemp', 1:'spechum', 2:'airpres', 3:'forcingUGRD', 4:'windspd',\n",
    "         5:'LWRadAtm',6:'forcingCONVfrac', 7:'forcingCAPE', 8:'forcingPEVAP', 9:'pptrate', 10:'SWRadAtm'}\n",
    "fvars = {0:'airpres', 1:'airtemp', 2:'pptrate', 3:'spechum', 4:'windspd', 5:'LWRadAtm',6:'SWRadAtm'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be calculating hours starting from: \n",
      "1979-01-01 13:00:00\n"
     ]
    }
   ],
   "source": [
    "# Set start and end data information for the GRIB/NetCDF forcing data.\n",
    "yearStart  = 1979\n",
    "monthStart = 1 \n",
    "dayStart   = 1 \n",
    "hourStart  = 13\n",
    "startDateTime = dt.datetime(yearStart, monthStart, dayStart, hour = hourStart)\n",
    "print(\"Will be calculating hours starting from: \")\n",
    "print(startDateTime)\n",
    "dayOfYearStart = dt.datetime.date(startDateTime).timetuple().tm_yday\n",
    "yearEnd  = 1979\n",
    "monthEnd = 12\n",
    "dayEnd   = 31 \n",
    "hourEnd  = 23\n",
    "endDateTime = dt.datetime(yearEnd, monthEnd, dayEnd, hour = hourEnd)\n",
    "dayOfYearEnd = dt.datetime.date(endDateTime).timetuple().tm_yday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the directory, but will change each day and year.\n",
    "mainDirectory = '/home/NearingLab/data/nldas/grib/NLDAS2.FORCING/'\n",
    "startDirectory = mainDirectory + str(yearStart)  + \"/\" \\\n",
    "    + str(\"{:03d}\".format(dayOfYearStart))  + \"/\"\n",
    "endDirectory = mainDirectory + str(yearEnd)  + \"/\" \\\n",
    "    + str(\"{:03d}\".format(dayOfYearEnd))  + \"/\"\n",
    "filePrefix = 'NLDAS_FORA0125_H.A'\n",
    "fileSufix = '.002.grb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the data and time to fine the correct file in this name format\n",
    "startFileDateTime = npt.dateForFile(yearStart, monthStart, dayStart, hourStart)\n",
    "endFileDateTime = npt.dateForFile(yearEnd, monthEnd, dayEnd, hourEnd)\n",
    "#Add prefix and sufix to the date to create the whole file name.\n",
    "startFile = npt.getFileName(startFileDateTime, startDirectory, \"A\")\n",
    "endFile = npt.getFileName(endFileDateTime, endDirectory, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get the GRIB time for the first and last files\n",
    "#Start the loop at the first date in the files.\n",
    "year1, month1, day1, hour1 = npt.dateFromGRIB(startFile)\n",
    "t = dt.datetime(year1, month1, day1, hour=hour1)\n",
    "#Then have the loop run until the last file date.\n",
    "year2, month2, day2, hour2 = npt.dateFromGRIB(endFile)\n",
    "endTime = dt.datetime(year2, month2, day2, hour=hour2)\n",
    "# Set timestep to move forward, to run through the files\n",
    "deltime = dt.timedelta(hours=1)\n",
    "# Estimate the number of hours in the record\n",
    "H = endTime - t # (t = startDateTime)\n",
    "# Convert the time difference to hours) \n",
    "H = int(H.total_seconds()/60/60) + 1\n",
    "time_series = [0 for x in range(H)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8747"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list of all the times to loop through\n",
    "dates = [startDateTime + deltime*h for h in range(H)]\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d5d3aa7fa640df9f90afc97e5da336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=464.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# find all the masked cells before the main loop, and avoid them\n",
    "ixy = -1 # Start at -1, so when we add the first value before the mask check, it goes to 0\n",
    "xy_list = []\n",
    "for x in tqdm(range(464)):\n",
    "    for y in range(224):            \n",
    "        ixy+=1 # lat/lon from the 1D arrays that correspond to these indices\n",
    "        if np.ma.is_masked(gbf_temp[11].values[y, x]): # Skip masked cells, takes .0044 seconds\n",
    "            continue\n",
    "        xy_list.append(ixy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f9134e176b4ce9a4fe35295702de00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80439.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up a dictionary for each cell, keys named from lat-lon\n",
    "# Will be filled in with data from the grib files\n",
    "G = {}\n",
    "for ixy in tqdm(xy_list):\n",
    "    xy = npt.name_xy(ixy, lats, lons)\n",
    "    G[xy] = npt.setForcingLists(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a502f3ffd4df493aa90869e2cebab4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8747.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main loop through the GRIB files by one hour intervals. open, extract, write, save\n",
    "# Main loop through the NetCDF files by one hour intervals. \n",
    "# iH: Index to use for filling forcing data list.\n",
    "for iH, t in enumerate(tqdm(dates)):\n",
    "\n",
    "    hoursSinceStartDate = t - startDateTime\n",
    "    hoursSinceStartDate = int(hoursSinceStartDate.total_seconds()/60/60)\n",
    "    time_series[iH] = float(hoursSinceStartDate)\n",
    "\n",
    "    # The files have both A and B versions.\n",
    "    AB = \"A\"\n",
    "    # Set the strings for the file name\n",
    "    iYear, iMonth, iDay, iHour = npt.getValuesFromDateTime(t)\n",
    "    # Get the datetime stuff in strings to be used in the NetCDF file call.\n",
    "    dateTime4File = npt.dateForFile(iYear, iMonth, iDay, iHour)\n",
    "    # Need to change the directory to reflect the loop data\n",
    "    directory = npt.changeDirectory(t, grib_dir)\n",
    "    # Put the file name together, this includes the full path\n",
    "    fileName = npt.getFileName(dateTime4File, directory, AB)\n",
    "    # Open the file for this particular data & time.\n",
    "    try:\n",
    "        gbf = pg.open(fileName)\n",
    "    except:\n",
    "        # skip the file\n",
    "        print('File not found: \\n',fileName)\n",
    "        continue\n",
    "    \n",
    "#####################################################################\n",
    "#####################################################################\n",
    "#####   THIS IS A MAJOR CHANGE, AND IS NOT WORKING YET  #############\n",
    "    g = extractGrib(gbf, xy_list, nrows, ncols)\n",
    "#####   NEED TO COLLECT DATA IN VECTOR, THEN ASSIGN TO THE CELL SOMEHOW\n",
    "\n",
    "\n",
    "    # Looping takes too long. Need to get all values in vector\n",
    "    # through x,y 1D indices.\n",
    "    for ixy in xyloop:\n",
    "\n",
    "        xy = npt.name_xy(ixy, lats, lons)\n",
    "        \n",
    "        # Need to get the two dimensional x,y values from the 1D xy\n",
    "        x, y = np.unravel_index(ixy, (ncols,nrows))\n",
    "                \n",
    "        # Fill in the main Grib dictionary.\n",
    "        for iv, v in enumerate(fvars):\n",
    "            G[xy][fvars[v]][iH] = g[fvars[v]][ixy]\n",
    "\n",
    "# Save the whole data periodically.\n",
    "save_G_name = write_dir+'grib_export.pkl'\n",
    "with open(save_G_name,'wb') as f:\n",
    "    pkl.dump(G, f)\n",
    "os.chmod(save_G_name, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractGrib(g, xy_list, nrows, ncols, verbose=False):\n",
    "    # 1:11:11 TMP, 2-m above ground Temperature [K]\n",
    "    airtemp = np.array(g[1].values.reshape(nrows*ncols)[xy_list])\n",
    "\n",
    "    # 2:51:51 SPFH, 2-m above ground Specific humidity [kg/kg]\n",
    "    spechum = np.array(g[2].values.reshape(nrows*ncols)[xy_list])\n",
    "    # 3:1:1 PRES, Surface pressure [Pa]\n",
    "    airpres = np.array(g[3].values.reshape(nrows*ncols)[xy_list])\n",
    "    # 4:33:33 UGRD, 10-m above ground Zonal wind speed [m/s]\n",
    "    forcingUGRD = np.array(g[4].values.reshape(nrows*ncols)[xy_list])\n",
    "    # 5:34:34 VGRD, 10-m above ground Meridonal wind speed [m/s]\n",
    "    windspd = np.array(g[5].values.reshape(nrows*ncols)[xy_list])\n",
    "    # 6:205:205 DLWRF,  Longwave radiation flux downwards [W/m^2]\n",
    "    LWRadAtm = np.array(g[6].values.reshape(nrows*ncols)[xy_list])\n",
    "    # 7:153:153 CONVfrac, Frac of total precip convective\n",
    "    forcingCONVfrac = np.array(g[7].values.reshape(nrows*ncols)[xy_list])\n",
    "    # 8:157:157 CAPE, 180-mb above ground Convective Available Potential Energy\n",
    "    forcingCAPE = np.array(g[8].values.reshape(nrows*ncols)[xy_list])\n",
    "        \n",
    "    # PEVAP, Potential evaporation hourly total   MAYBE: Adiabatic tendency of temperature?\n",
    "    forcingPEVAP = np.array(g[9].values.reshape(nrows*ncols)[xy_list])\n",
    "        \n",
    "    # 10:61:61 APCP, Precipitation hourly total [kg/m^2/hr]\n",
    "    pptrate = np.array(g[10].values.reshape(nrows*ncols)[xy_list]) / 60 / 60\n",
    "        \n",
    "    # 11:204:204 DSWRF, Shortwave radiation flux downwards (surface) [W/m^2]\n",
    "    SWRadAtm = np.array(g[11].values.reshape(nrows*ncols)[xy_list])\n",
    "                 \n",
    "    G={'airtemp':airtemp, 'spechum':spechum, 'airpres':airpres, 'forcingUGRD':forcingUGRD, \n",
    "       'windspd':windspd, 'LWRadAtm':LWRadAtm, 'forcingCONVfrac':forcingCONVfrac, \n",
    "       'forcingCAPE':forcingCAPE,'forcingPEVAP':forcingPEVAP, 'pptrate':pptrate, 'SWRadAtm':SWRadAtm}\n",
    "           \n",
    "    if verbose:\n",
    "        print(\"TMP, 2-m above ground Temperature [K]: {}\".format(airtemp))\n",
    "        print(\"SPFH, 2-m above ground Specific humidity [kg/kg]: {}\".format(spechum))\n",
    "        print(\"PRES, Surface pressure [Pa]: {}\".format(airpres))\n",
    "        print(\"UGRD, 10-m above ground Zonal wind speed [m/s]: {}\".format(forcingUGRD))\n",
    "        print(\"VGRD, 10-m above ground Meridonal wind speed [m/s]: {}\".format(windspd))\n",
    "        print(\"DLWRF,  Longwave radiation flux downwards (surface) [W/m^2]: {}\".format(LWRadAtm))\n",
    "        print(\"CONVfrac, Fraction of total precipitation that is convective: {}\".format(forcingCONVfrac))\n",
    "        print(\"CAPE, 180-mb above ground Convective Available Potential Energy: {}\".format(forcingCAPE))\n",
    "        print(\"PEVAP, Potential evaporation hourly total: {}\".format(forcingPEVAP))\n",
    "        print(\"APCP, Precipitation hourly total [kg/m^2/hr]: {}\".format(pptrate))\n",
    "        print(\"DSWRF, Shortwave radiation flux downwards (surface) [W/m^2]: {}\".format(SWRadAtm))\n",
    "            \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191f07dc442a49ca9fc5279053459cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80439.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the forcing data for each cell, individually\n",
    "for ixy in tqdm(xy_list):\n",
    "    xy = npt.name_xy(ixy, lats, lons)\n",
    "    x, y = np.unravel_index(ixy, (ncols,nrows))\n",
    "    lat=lats[ixy]\n",
    "    lon=lons[ixy]\n",
    "    timestp=3600 #seconds\n",
    "    # Write the NetCDF forcing data file.\n",
    "    fname = \"{}-{}\".format(lat, -lon)\n",
    "    forcingDataName = write_dir + fname +'.nc'\n",
    "    forcing = nc.Dataset(forcingDataName, 'w', format='NETCDF4_CLASSIC')\n",
    "    forcing.title = \"NLDAS forcing \"+fname\n",
    "    forcing.description = 'NLDAS forcing data for '+fname\n",
    "    forcing = npt.fillForcing(forcing, H, lat, lon, timestp, time_series, \n",
    "        G[xy]['SWRadAtm'], \n",
    "        G[xy]['LWRadAtm'], \n",
    "        G[xy]['airpres'], \n",
    "        G[xy]['airtemp'], \n",
    "        G[xy]['pptrate'], \n",
    "        G[xy]['spechum'], \n",
    "        G[xy]['windspd'])\n",
    "    os.chmod(forcingDataName, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4_CLASSIC data model, file format HDF5):\n",
      "    title: NLDAS forcing 25.063-124.938\n",
      "    description: NLDAS forcing data for 25.063-124.938\n",
      "    dimensions(sizes): hru(1), time(8747)\n",
      "    variables(dimensions): float32 latitude(hr), float32 longitude(hr), int32 data_step(), float64 time(time), float32 LWRadAtm(time,hr), float32 SWRadAtm(time,hr), float32 airpres(time,hr), float32 airtemp(time,hr), float32 pptrate(time,hr), float32 spechum(time,hr), float32 windspd(time,hr)\n",
      "    groups: \n"
     ]
    }
   ],
   "source": [
    "ds = nc.Dataset(forcingDataName)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.   ],\n",
       "       [  0.   ],\n",
       "       [168.384],\n",
       "       ...,\n",
       "       [634.256],\n",
       "       [492.432],\n",
       "       [331.12 ]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ds.variables['SWRadAtm'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25.063, -124.938'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npt.name_xy(0, lats, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
