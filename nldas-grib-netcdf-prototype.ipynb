{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is my prototyping platform for the code to extract a timeseries of data from the NLDAS GRB files and store them in NetCDF format.\n",
    "## There should be a .py script with a similar name that runs the finished code on HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import netCDF4 as nc # http://unidata.github.io/netcdf4-python/\n",
    "import pygrib as pg\n",
    "import sys\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle as pkl\n",
    "import nldas_pygrib_tools as npt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_year = 1980\n",
    "year_splits = 10\n",
    "recover = False\n",
    "search_xy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the NetCDF forcing data file.\n",
    "proj_dir = '/home/NearingLab/projects/jmframe/nldas-grib/'\n",
    "grib_dir = '/home/NearingLab/data/nldas/grib/NLDAS2.FORCING/'\n",
    "write_dir = '/home/NearingLab/data/nldas/netcdf-single-cells/'+str(nc_year)+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an example file\n",
    "fname = grib_dir + '1979/001/' + 'NLDAS_FORA0125_H.A'+ str(nc_year) +'0101.0000.002.grb'\n",
    "if yearStart == 1979:\n",
    "    fname = grib_dir + str(nc_year) +'/001/' + 'NLDAS_FORA0125_H.A19790101.0000.002.grb'\n",
    "gbf_temp = pg.open(fname)\n",
    "lats = gbf_temp[1].latitudes\n",
    "lons = gbf_temp[1].longitudes\n",
    "nrows = gbf_temp[11].values.shape[0]\n",
    "ncols = gbf_temp[11].values.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvars = {0:'airtemp', 1:'spechum', 2:'airpres', 3:'forcingUGRD', 4:'windspd',\n",
    "         5:'LWRadAtm',6:'forcingCONVfrac', 7:'forcingCAPE', 8:'forcingPEVAP', 9:'pptrate', 10:'SWRadAtm'}\n",
    "fvars = {0:'airpres', 1:'airtemp', 2:'pptrate', 3:'spechum', 4:'windspd', 5:'LWRadAtm',6:'SWRadAtm'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be calculating hours starting from: \n",
      "1980-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Set start and end data information for the GRIB/NetCDF forcing data.\n",
    "yearStart  = nc_year\n",
    "monthStart = 1 \n",
    "dayStart   = 1\n",
    "if yearStart == 1979:\n",
    "    hourStart = 13\n",
    "else:\n",
    "    hourStart = 0\n",
    "startDateTime = dt.datetime(yearStart, monthStart, dayStart, hour = hourStart)\n",
    "print(\"Will be calculating hours starting from: \")\n",
    "print(startDateTime)\n",
    "dayOfYearStart = dt.datetime.date(startDateTime).timetuple().tm_yday\n",
    "yearEnd  = nc_year\n",
    "monthEnd = 12\n",
    "dayEnd   = 31 \n",
    "hourEnd  = 23\n",
    "endDateTime = dt.datetime(yearEnd, monthEnd, dayEnd, hour = hourEnd)\n",
    "dayOfYearEnd = dt.datetime.date(endDateTime).timetuple().tm_yday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the directory, but will change each day and year.\n",
    "mainDirectory = '/home/NearingLab/data/nldas/grib/NLDAS2.FORCING/'\n",
    "startDirectory = mainDirectory + str(yearStart)  + \"/\" \\\n",
    "    + str(\"{:03d}\".format(dayOfYearStart))  + \"/\"\n",
    "endDirectory = mainDirectory + str(yearEnd)  + \"/\" \\\n",
    "    + str(\"{:03d}\".format(dayOfYearEnd))  + \"/\"\n",
    "filePrefix = 'NLDAS_FORA0125_H.A'\n",
    "fileSufix = '.002.grb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the data and time to fine the correct file in this name format\n",
    "startFileDateTime = npt.dateForFile(yearStart, monthStart, dayStart, hourStart)\n",
    "endFileDateTime = npt.dateForFile(yearEnd, monthEnd, dayEnd, hourEnd)\n",
    "#Add prefix and sufix to the date to create the whole file name.\n",
    "startFile = npt.getFileName(startFileDateTime, startDirectory, \"A\")\n",
    "endFile = npt.getFileName(endFileDateTime, endDirectory, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get the GRIB time for the first and last files\n",
    "#Start the loop at the first date in the files.\n",
    "year1, month1, day1, hour1 = npt.dateFromGRIB(startFile)\n",
    "t = dt.datetime(year1, month1, day1, hour=hour1)\n",
    "#Then have the loop run until the last file date.\n",
    "year2, month2, day2, hour2 = npt.dateFromGRIB(endFile)\n",
    "endTime = dt.datetime(year2, month2, day2, hour=hour2)\n",
    "# Set timestep to move forward, to run through the files\n",
    "deltime = dt.timedelta(hours=1)\n",
    "# Estimate the number of hours in the record\n",
    "H = endTime - t # (t = startDateTime)\n",
    "# Convert the time difference to hours) \n",
    "H = int(H.total_seconds()/60/60) + 1\n",
    "time_series = [0 for x in range(H)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8784"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list of all the times to loop through\n",
    "dates = [startDateTime + deltime*h for h in range(H)]\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the masked cells before the main loop, and avoid them\n",
    "list_fname = 'unmasked_xy_indices.txt'\n",
    "if search_xy:\n",
    "    ixy = -1 # Start at -1, so when we add the first value before the mask check, it goes to 0\n",
    "    xy_list = []\n",
    "    for x in tqdm(range(464)):\n",
    "        for y in range(224):            \n",
    "            ixy+=1 # lat/lon from the 1D arrays that correspond to these indices\n",
    "            if np.ma.is_masked(gbf_temp[11].values[y, x]): # Skip masked cells, takes .0044 seconds\n",
    "                continue\n",
    "            xy_list.append(ixy)\n",
    "    np.savetxt(list_fname, xy_list)\n",
    "elif os.path.exists(list_fname):\n",
    "    xy_list = list(np.genfromtxt(list_fname))\n",
    "    for i, ixy in enumerate(xy_list):\n",
    "        xy_list[i] = int(xy_list[i])\n",
    "else:\n",
    "    print(\"xy_list not working\")\n",
    "N = len(xy_list)\n",
    "num_in_split = int(N/year_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b573c53fc5fc49dc9079cc08aa53097e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940b990a9f8b4816bbef597e17d872ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cec69c111bb4c95b4a62e26cf705dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d004b84566d14399afe503a7eae20967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14b0b74b02a435f97a128db3d58478d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2367fc55b4484cbf9b3a933074e7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2704f71692a847698296811d40b53042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5461256b59cf446bb02eddafa4f6b3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505f8b76ba8e4ed8b169bf81f9787856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d381df5ba2741d6b4817e1cb5de95c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8052.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up a dictionary for each cell, keys named from lat-lon\n",
    "# Will be filled in with data from the grib files\n",
    "start_time_index = 0\n",
    "# if recover:\n",
    "#     with open(G_pkl_file,'rb') as f:\n",
    "#         G = pkl.read(f)\n",
    "#     # Find where the saved file left off:\n",
    "#     for iH, t in enumerate(tqdm(dates)):\n",
    "#         i=0\n",
    "#         ixy=xy_list[i]\n",
    "#         xy = npt.name_xy(ixy, lats, lons)\n",
    "#         v = fvars[0]\n",
    "#         if np.isnan(G[xy][fvars[v]][iH]):\n",
    "#             continue\n",
    "#         else:\n",
    "#             start_time_index = iH\n",
    "#             break\n",
    "# else:\n",
    "G = {i:{} for i in range(year_splits)}\n",
    "for isplit in range(year_splits):\n",
    "    if isplit+1==year_splits: # Last split ends at i=N\n",
    "        for i, ixy in enumerate(tqdm(xy_list[isplit*int(N/year_splits):N])):\n",
    "            xy = npt.name_xy(ixy, lats, lons)\n",
    "            G[isplit][xy] = npt.setForcingLists(H)\n",
    "    else:\n",
    "        for i, ixy in enumerate(tqdm(xy_list[isplit*int(N/year_splits):(isplit+1)*int(N/year_splits)])):\n",
    "            xy = npt.name_xy(ixy, lats, lons)\n",
    "            G[isplit][xy] = npt.setForcingLists(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop through the GRIB files by one hour intervals. open, extract, write, save\n",
    "# Main loop through the NetCDF files by one hour intervals. \n",
    "# iH: Index to use for filling forcing data list.\n",
    "for iH, t in enumerate(tqdm(dates)):\n",
    "    \n",
    "    # start from where the last save was made\n",
    "#     if recover:\n",
    "#         if iH < start_time_index:\n",
    "#             continue\n",
    "    \n",
    "    hoursSinceStartDate = t - startDateTime\n",
    "    hoursSinceStartDate = int(hoursSinceStartDate.total_seconds()/60/60)\n",
    "    time_series[iH] = float(hoursSinceStartDate)\n",
    "\n",
    "    # The files have both A and B versions.\n",
    "    AB = \"A\"\n",
    "    # Set the strings for the file name\n",
    "    iYear, iMonth, iDay, iHour = npt.getValuesFromDateTime(t)\n",
    "    # Get the datetime stuff in strings to be used in the NetCDF file call.\n",
    "    dateTime4File = npt.dateForFile(iYear, iMonth, iDay, iHour)\n",
    "    # Need to change the directory to reflect the loop data\n",
    "    directory = npt.changeDirectory(t, grib_dir)\n",
    "    # Put the file name together, this includes the full path\n",
    "    fileName = npt.getFileName(dateTime4File, directory, AB)\n",
    "    # Open the file for this particular data & time.\n",
    "    try:\n",
    "        gbf = pg.open(fileName)\n",
    "    except:\n",
    "        # skip the file\n",
    "        print('File not found: \\n',fileName)\n",
    "        continue\n",
    "    \n",
    "    #####################################################################\n",
    "    #####   THIS IS the function to actually get the forcing data  ######\n",
    "    g = npt.extractGrib(gbf, xy_list, nrows, ncols)\n",
    "\n",
    "    # Looping takes too long. Need to get all values in vector\n",
    "    # through x,y 1D indices.\n",
    "    for i, ixy in enumerate(xy_list):\n",
    "        \n",
    "        # Calculate the split in the list from i.        \n",
    "        isplit = int(np.floor(i/num_in_split))\n",
    "        if isplit >= year_splits:\n",
    "            isplit = year_splits - 1\n",
    "\n",
    "        xy = npt.name_xy(ixy, lats, lons)\n",
    "        \n",
    "        # Need to get the two dimensional x,y values from the 1D xy\n",
    "        x, y = np.unravel_index(ixy, (ncols,nrows))\n",
    "                \n",
    "        # Fill in the main Grib dictionary.\n",
    "        for iv, v in enumerate(fvars):\n",
    "            G[isplit][xy][fvars[v]][iH] = g[fvars[v]][i]\n",
    "               \n",
    "    # Save the whole data periodically.\n",
    "    if (iH>0) and not iH % 2000 or t == dates[-1]:\n",
    "        for isplit in range(year_splits):\n",
    "            G_pkl_file = write_dir+'grib_export_'+str(isplit)+'.pkl'\n",
    "            print('writing G[{}] at time: {}'.format(isplit, t))\n",
    "            with open(G_pkl_file,'wb') as f:\n",
    "                pkl.dump(G[isplit], f)\n",
    "            os.chmod(G_pkl_file, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(list(G[i].keys())) for i in range(year_splits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G[9].keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npt.name_xy(xy_list[-1], lats, lons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, v in enumerate(fvars):\n",
    "    print(G[xy][fvars[v]][iH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the forcing data for each cell, individually\n",
    "for i, ixy in enumerate(tdqm(xy_list)):\n",
    "    # Calculate the split in the list from i.        \n",
    "    isplit = int(np.floor(i/num_in_split))\n",
    "    if isplit >= year_splits:\n",
    "        isplit = year_splits - 1\n",
    "    xy = npt.name_xy(ixy, lats, lons)\n",
    "    x, y = np.unravel_index(ixy, (ncols,nrows))\n",
    "    lat=lats[ixy]\n",
    "    lon=lons[ixy]\n",
    "    timestp=3600 #seconds\n",
    "    # Write the NetCDF forcing data file.\n",
    "    fname = \"{}-{}\".format(lat, -lon)\n",
    "    forcingDataName = write_dir + fname +'.nc'\n",
    "    forcing = nc.Dataset(forcingDataName, 'w', format='NETCDF4_CLASSIC')\n",
    "    forcing.title = \"NLDAS forcing \"+fname\n",
    "    forcing.description = 'NLDAS forcing data for '+fname\n",
    "    forcing = npt.fillForcing(forcing, H, lat, lon, timestp, time_series, \n",
    "        G[isplit][xy]['SWRadAtm'], \n",
    "        G[isplit][xy]['LWRadAtm'], \n",
    "        G[isplit][xy]['airpres'], \n",
    "        G[isplit][xy]['airtemp'], \n",
    "        G[isplit][xy]['pptrate'], \n",
    "        G[isplit][xy]['spechum'], \n",
    "        G[isplit][xy]['windspd'], year)\n",
    "    os.chmod(forcingDataName, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nc.Dataset(write_dir+'25.063-124.938.nc')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvar = 'time'\n",
    "arr = np.zeros(np.array(ds.variables[fvar][:]).shape[0])\n",
    "for i in range(np.array(ds.variables[fvar][:]).shape[0]):\n",
    "    arr[i] = np.array(ds.variables[fvar][:])[i]\n",
    "print(arr[0:10])\n",
    "print(arr[-10:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npt.name_xy(0, lats, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.variables['time'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
